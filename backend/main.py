from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import google.generativeai as genai
import os
from dotenv import load_dotenv
import asyncio
from google.generativeai.types import GenerationConfig, Tool, FunctionDeclaration
from fastapi import HTTPException
from contract_generator import generate_contract_cascade

load_dotenv()

app = FastAPI()

# Configuration du CORS pour autoriser les requ√™tes du frontend
origins = [
    "http://localhost",
    "http://localhost:5173", # Port par d√©faut de Vite/React
    "http://localhost:5174", # Port configur√© pour ce projet
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Configuration de l'API Gemini avec les variables d'environnement
try:
    GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
    if not GEMINI_API_KEY:
        raise ValueError("La cl√© d'API Gemini (GEMINI_API_KEY) n'est pas d√©finie dans le fichier .env")
    genai.configure(api_key=GEMINI_API_KEY)
except Exception as e:
    print(f"Erreur lors de la configuration de l'API Gemini : {e}")

class ChatRequest(BaseModel):
    text: str
    history: list = []

class GenerateLawyerResponseRequest(BaseModel):
    history: list = []

# D√©finition de notre outil personnalis√© pour la cascade de g√©n√©ration
lancer_cascade_generation_tool = Tool(
    function_declarations=[
        FunctionDeclaration(
            name="lancer_cascade_generation",
            description="√Ä n'utiliser que lorsque l'utilisateur a fourni toutes les informations n√©cessaires et confirme explicitement vouloir g√©n√©rer le document final."
        )
    ]
)

# The Master Prompt that guides the AI
MASTER_PROMPT = """
You are an expert business law assistant specializing in contract top tier contract drafting.
Your goal is to help top tier lawyers draft complex legal documents following a precise and complete methodology.
Don't loose time in greeting and politeness with the lawyers, but respond directly to their request, as a first message is already present in the interface, so the conversation is always already begun with the initial message: "For this new mandate, what is the primary strategic objective your client is seeking to achieve?" So the first message you will receive from the lawyer will probably answer this question.

1. Dialogue and collection Your goal is to collect ALL information necessary to write a very complete, professional legal document, not a draft.

(Of course, this includes party details, addresses, signatories, and every element required for a binding, but you must start with the more clever questions)

2. **Await confirmation:** Once you have gathered absolutely all essential information, provide a comprehensive summary and ask for clear confirmation, such as "Shall we proceed with generating the document?".
3. **Tool call:** Only after receiving this explicit confirmation, you MUST call the `lancer_cascade_generation` tool. Write nothing else in your response.
"""

LAWYER_SIMULATOR_PROMPT = """
**Your Role:**
You are an experienced business lawyer in dialogue with your AI assistant (CounselAI).
Your assistant asks you questions to gather all the information necessary to draft a contract.

**Your Mission:**
Answer **directly** to the assistant
Provide concrete, plausible, and concise information in English.
Add no greetings or superfluous phrases. NEVER ask questions in return.
"""

@app.get("/")
def read_root():
    return {"status": "backend is running"}

async def stream_generator(model, message_text):
    """
    G√©n√©rateur asynchrone qui produit les morceaux de la r√©ponse de l'IA.
    """
    try:
        # On active le streaming dans l'appel √† l'API
        stream = await model.generate_content_async(message_text, stream=True)
        async for chunk in stream:
            if chunk.text:
                yield chunk.text
                await asyncio.sleep(0.01) # Petite pause pour permettre l'envoi
    except Exception as e:
        print(f"Erreur pendant le streaming : {e}")
        yield f"Erreur de communication avec le mod√®le d'IA : {e}"

@app.post("/api/generate_lawyer_response")
async def generate_lawyer_response(request: GenerateLawyerResponseRequest):
    """
    G√©n√®re une r√©ponse d'avocat simul√©e bas√©e sur l'historique de la conversation.
    """
    try:
        # Utilise un mod√®le d√©di√© avec le prompt du simulateur d'avocat
        lawyer_model = genai.GenerativeModel(
            model_name=os.getenv("GEMINI_MODEL_NAME", "gemini-pro"),
            system_instruction=LAWYER_SIMULATOR_PROMPT
        )
        
        chat_session = lawyer_model.start_chat(history=request.history)
        # On ne streame pas, on veut la r√©ponse compl√®te directement
        response = await chat_session.send_message_async("R√©pondez √† la derni√®re question de l'assistant en vous basant sur l'historique.")
        
        return {"response": response.text}

    except Exception as e:
        print(f"Erreur lors de la g√©n√©ration de la r√©ponse de l'avocat : {e}")
        raise HTTPException(status_code=500, detail="Erreur interne du serveur")

class GenerateContractRequest(BaseModel):
    history: list

@app.post("/api/generate_contract")
async def generate_contract(request: GenerateContractRequest):
    """
    Endpoint pour g√©n√©rer un contrat bas√© sur l'historique de conversation.
    Utilise l'architecture en cascade avec des LLMs sp√©cialis√©s.
    """
    print(f"üî• /api/generate_contract appel√© avec {len(request.history)} messages dans l'historique")
    
    try:
        # Utiliser la cascade de g√©n√©ration avec le mod√®le depuis l'environnement
        print(f"üöÄ Lancement de generate_contract_cascade...")
        result = await generate_contract_cascade(
            conversation_history=request.history,
            api_key=GEMINI_API_KEY,
            model_name=os.getenv("GEMINI_MODEL_NAME", "gemini-pro")
        )
        
        return {
            "status": "success",
            "contract_markdown": result['markdown'],
            "contract_html": result['html'],
            "extracted_data": result['data']
        }
    
    except Exception as e:
        print(f"Erreur lors de la g√©n√©ration du contrat : {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/chat")
async def chat(request: ChatRequest):
    """
    Endpoint de chat qui g√®re la logique du Master Prompt, des outils, et de l'historique.
    """
    async def stream_response_generator():
        try:
            model = genai.GenerativeModel(
                model_name=os.getenv("GEMINI_MODEL_NAME", "gemini-pro"),
                system_instruction=MASTER_PROMPT,
                tools=[lancer_cascade_generation_tool]
            )
            
            chat_session = model.start_chat(history=request.history)
            response = await chat_session.send_message_async(request.text, stream=True)

            # Boucle de streaming unique et propre pour corriger le bug de r√©p√©tition.
            async for chunk in response:
                try:
                    # V√©rifier d'abord les appels de fonction
                    if hasattr(chunk, 'candidates') and chunk.candidates:
                        candidate = chunk.candidates[0]
                        if hasattr(candidate.content, 'parts') and candidate.content.parts:
                            for part in candidate.content.parts:
                                if hasattr(part, 'function_call') and part.function_call:
                                    tool_name = part.function_call.name
                                    print(f"D√©tection d'un appel √† l'outil : {tool_name}")
                                    yield f"TOOL_CALL:{tool_name}"
                                    return  # Arr√™ter le streaming apr√®s l'appel d'outil
                    
                    # Ensuite v√©rifier le texte
                    if hasattr(chunk, 'text'):
                        try:
                            text = chunk.text
                            if text:
                                yield text
                                await asyncio.sleep(0.01)
                        except Exception as text_error:
                            # Ignorer les erreurs de texte et continuer
                            pass
                except Exception as chunk_error:
                    # Ignorer les chunks probl√©matiques et continuer
                    pass

        except Exception as e:
            print(f"Une erreur est survenue dans l'endpoint chat : {e}")
            yield f"Erreur critique inattendue: {str(e)}"

    return StreamingResponse(stream_response_generator(), media_type="text/plain") 